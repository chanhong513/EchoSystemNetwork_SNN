{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gIyynl9m9es"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TtY7Q4Lfm9et"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.matlib as npm\n",
        "# import scipy as sc\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import snntorch as snn\n",
        "from snntorch import utils\n",
        "from snntorch import spikegen\n",
        "import snntorch.functional as SF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSQ879mSm9eu"
      },
      "source": [
        "# Define Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Bx6LVy4Ym9eu"
      },
      "outputs": [],
      "source": [
        "class DeepESN():\n",
        "\n",
        "    def __init__(self, Nu, Nr, Nl, configs, device='cpu'):\n",
        "        \n",
        "        reservoirConf = configs.reservoirConf  # reservoir configurations\n",
        "\n",
        "\n",
        "        self.W = {}\n",
        "        self.Win = {}\n",
        "        #self.Gain = {}\n",
        "        #self.Bias = {}\n",
        "\n",
        "        self.Nu = Nu  # number of inputs\n",
        "        self.Nr = Nr  # number of units per layer\n",
        "        self.Nl = Nl  # number of layers\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.leaky = configs.snn.leaky\n",
        "        self.Gain = configs.snn.Gain\n",
        "        self.Bias = configs.snn.Bias\n",
        "        self.spike_train = configs.snn.spike_train\n",
        "        self.threshold = configs.snn.threshold\n",
        "        self.number_row_elements = round(reservoirConf.connectivity * Nr)\n",
        "        \n",
        "\n",
        "        # if reservoir neurons are not fully connected\n",
        "        if reservoirConf.connectivity < 1:\n",
        "            for layer in range(Nl):\n",
        "                self.W[layer] = torch.zeros((Nr, Nr), device=device)\n",
        "                for row in range(Nr):\n",
        "                    number_row_elements = round(reservoirConf.connectivity * Nr)\n",
        "                    row_elements = torch.randperm(Nr, device=device)[:number_row_elements]\n",
        "                    self.W[layer][row, row_elements] = torch.rand(number_row_elements, device=device)*2 - 1\n",
        "\n",
        "        # fully_connected reservoir neurons\n",
        "        else:\n",
        "            for layer in range(Nl):\n",
        "                self.W[layer] = torch.rand(number_row_elements, device=device)*2 - 1\n",
        "\n",
        "        # initialize layers\n",
        "        for layer in range(Nl):\n",
        "\n",
        "            # initializing weight vector\n",
        "            # size is extended by 1 due to the bias term\n",
        "            if layer == 0:\n",
        "                self.Win[layer] = (torch.rand((Nr, Nu), device=device)*2 - 1)\n",
        "            else:\n",
        "                self.Win[layer] = (torch.rand((Nr, Nr), device=device)*2 - 1)\n",
        "\n",
        "            # Ws = (1 - target_li) * torch.eye(Nr, device=device) + target_li * .W[layer]\n",
        "            # eig_value, _ = torch.linalg.eig(Ws)\n",
        "            # actual_rho = torch.max(torch.abs(eig_value))\n",
        "\n",
        "            # Ws = (Ws * target_rho) / actual_rho\n",
        "            # self.W[layer] = (target_li ** -1) * (Ws - (1. - target_li) * torch.eye(Nr, device=device))\n",
        "\n",
        "            #self.Gain[layer] = torch.ones((Nr, 1), device=device)\n",
        "            #self.Bias[layer] = torch.zeros((Nr, 1), device=device)\n",
        "\n",
        "    def computeLayerState(self, input, layer, initialStatesSpike, initialStatesLayer):\n",
        "\n",
        "        # Compute the input for the current layer\n",
        "        input = self.Win[layer]@input\n",
        "    \n",
        "        # Compute the new state using the leaky integration\n",
        "        state = self.Gain*((self.leaky)*initialStatesLayer + (input + (self.W[layer]@initialStatesSpike)))\n",
        "        # state = self.Gain*((1-self.leaky)*initialStatesLayer + 0.5*(input + (self.W[layer]@initialStatesSpike))) + self.Bias\n",
        "    \n",
        "        # Initialize the spike tensor\n",
        "        spk = torch.zeros(self.Nr, device=self.device)\n",
        "    \n",
        "        # Determine where the state exceeds the threshold\n",
        "        spk = (state > self.threshold).float()\n",
        "    \n",
        "        # Reset the state values that exceed the threshold to 0\n",
        "        state[state > self.threshold] =0\n",
        "        return state, spk\n",
        "\n",
        "\n",
        "    def computeState(self, inputs, initialStates=None):\n",
        "        spikes = []\n",
        "        states = []\n",
        "        # print(self.W)\n",
        "        # print(self.Win)\n",
        "        \n",
        "        for i_seq in range(len(inputs)):\n",
        "            spike, state = self.computeGlobalState(inputs[i_seq])\n",
        "            spikes.append(spike)\n",
        "            states.append(state)\n",
        "            if i_seq % 100 == 0:\n",
        "                print(\"Number of Calculated: \", i_seq)\n",
        "\n",
        "        # Convert the states list to a PyTorch tensor\n",
        "        return torch.stack(spikes).to(self.device), torch.stack(states).to(self.device)\n",
        "\n",
        "    def computeGlobalState(self, input):\n",
        "        state = torch.zeros((self.Nl * self.Nr), device=self.device)\n",
        "        spike = torch.zeros((self.Nl * self.Nr), device=self.device)\n",
        "        out_spk = torch.zeros((10, self.Nl * self.Nr), device=self.device)\n",
        "        out_state = torch.zeros((10, self.Nl * self.Nr), device=self.device)\n",
        "        \n",
        "        for step in range(self.spike_train):\n",
        "            for i in range(7):           \n",
        "                parsedinput = input[step, 4*i: 4*(i+1), :].flatten().to(self.device)\n",
        "                for layer in range(self.Nl):\n",
        "                    initialStatesLayer = state[layer*self.Nr: (layer+1)*self.Nr]\n",
        "                    initialStatesSpike = spike[layer*self.Nr: (layer+1)*self.Nr]\n",
        "                    state[layer*self.Nr:(layer+1)*self.Nr], spike[layer*self.Nr:(layer+1)*self.Nr] = self.computeLayerState(parsedinput, layer, initialStatesSpike, initialStatesLayer)\n",
        "                    parsedinput = state[layer * self.Nr:(layer + 1) * self.Nr]\n",
        "                if i == 6: \n",
        "                    out_spk[step] = spike\n",
        "                    out_state[step] = state\n",
        "                    \n",
        "        # print(torch.stack(out_spk, dim=0).shape)\n",
        "        # shape of out_spk is [5, 800, 1]\n",
        "        # Convert the state to a PyTorch tensor\n",
        "        return out_spk, out_state\n",
        "\n",
        "    \n",
        "    \n",
        "class Readout(nn.Module):\n",
        "    def __init__(self, Nr, Nl, configs):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(Nr*Nl, 10)\n",
        "        self.lif1 = snn.Leaky(beta=0.8, threshold=1, output=True)\n",
        "        self.spike_train = configs.snn.spike_train\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # initialize membrane potential\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        \n",
        "        spk1_rec = []\n",
        "        mem1_rec = []\n",
        "        \n",
        "        for step in range(self.spike_train):\n",
        "            cur1 = self.fc1(x[:, step,:])\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            spk1_rec.append(spk1)\n",
        "            mem1_rec.append(mem1)\n",
        "        \n",
        "        spk1_rec = torch.stack(spk1_rec, dim=0)\n",
        "        mem1_rec = torch.stack(mem1_rec, dim=0)\n",
        "            \n",
        "        return spk1_rec, mem1_rec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZsabiApm9eu"
      },
      "source": [
        "# Configurations Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KyCgsDb4m9eu"
      },
      "outputs": [],
      "source": [
        "class Struct(object): pass\n",
        "\n",
        "def config_CIFAR10(IP_indexes):\n",
        "\n",
        "    configs = Struct()\n",
        "\n",
        "\n",
        "    configs.reservoirConf = Struct()\n",
        "    configs.reservoirConf.connectivity = .3\n",
        "\n",
        "    configs.snn = Struct()\n",
        "    configs.snn.leaky = 0.8\n",
        "    configs.snn.Gain = 1\n",
        "    configs.snn.Bias = 0\n",
        "    configs.snn.threshold = 1\n",
        "    configs.snn.spike_train = 10\n",
        "\n",
        "    return configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6lcxW0m9ev"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHMJRvbbm9ev",
        "outputId": "8c6518a1-01eb-4d95-b2ee-16cc03f5f458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First train sample shape: torch.Size([1, 28, 28])\n",
            "First train label: 5\n",
            "Training dataset size: 48000\n",
            "Validation dataset size: 12000\n",
            "First input sample shape: torch.Size([10, 28, 28])\n",
            "First target sample shape: torch.Size([1])\n",
            "Total dataset size (after adding validation): 60000\n",
            "Total dataset size (after adding test): 70000\n",
            "input shape: torch.Size([70000, 10, 28, 28])\n",
            "target shape: torch.Size([70000, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import functools\n",
        "\n",
        "steps = 10\n",
        "\n",
        "class Struct:\n",
        "    pass\n",
        "\n",
        "def load_MNIST():\n",
        "    # Load the MNIST dataset\n",
        "    transform = transforms.ToTensor()\n",
        "    train_dataset = datasets.MNIST('./datasets', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST('./datasets', train=False, download=True, transform=transform)\n",
        "\n",
        "    print(\"First train sample shape:\", train_dataset[0][0].shape)\n",
        "    print(\"First train label:\", train_dataset[0][1])\n",
        "\n",
        "    # Split train_dataset into training and validation sets\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    validation_size = len(train_dataset) - train_size\n",
        "    train_dataset, validation_dataset = torch.utils.data.random_split(train_dataset, [train_size, validation_size])\n",
        "\n",
        "    print(\"Training dataset size:\", len(train_dataset))\n",
        "    print(\"Validation dataset size:\", len(validation_dataset))\n",
        "\n",
        "    # Prepare the dataset object\n",
        "    dataset = Struct()\n",
        "    dataset.name = 'MNIST'\n",
        "    dataset.inputs = torch.stack([spikegen.rate(train_dataset[i][0].squeeze().T, num_steps=steps) for i in range(len(train_dataset))])\n",
        "    dataset.targets = torch.stack([torch.tensor(train_dataset[i][1]) for i in range(len(train_dataset))]).view(-1, 1)\n",
        "\n",
        "    print(\"First input sample shape:\", dataset.inputs[0].shape)\n",
        "    print(\"First target sample shape:\", dataset.targets[0].shape)\n",
        "\n",
        "    # Add validation data\n",
        "    validation_inputs = torch.stack([spikegen.rate(validation_dataset[i][0].squeeze().T, num_steps=steps) for i in range(len(validation_dataset))])\n",
        "    validation_targets = torch.stack([torch.tensor(validation_dataset[i][1]) for i in range(len(validation_dataset))]).view(-1, 1)\n",
        "    dataset.inputs = torch.cat((dataset.inputs, validation_inputs))\n",
        "    dataset.targets = torch.cat((dataset.targets, validation_targets))\n",
        "\n",
        "    print(\"Total dataset size (after adding validation):\", len(dataset.inputs))\n",
        "\n",
        "    # Add test data\n",
        "    test_inputs = torch.stack([spikegen.rate(test_dataset[i][0].squeeze().T, num_steps=steps) for i in range(len(test_dataset))])\n",
        "    test_targets = torch.stack([torch.tensor(test_dataset[i][1]) for i in range(len(test_dataset))]).view(-1, 1)\n",
        "    dataset.inputs = torch.cat((dataset.inputs, test_inputs))\n",
        "    dataset.targets = torch.cat((dataset.targets, test_targets))\n",
        "\n",
        "    print(\"Total dataset size (after adding test):\", len(dataset.inputs))\n",
        "\n",
        "    print(\"input shape:\", dataset.inputs.shape)\n",
        "    print(\"target shape:\", dataset.targets.shape)\n",
        "\n",
        "    # Input dimension\n",
        "    Nu = 28*4\n",
        "\n",
        "\n",
        "\n",
        "    # Define indexes for training, validation, and test sets\n",
        "    TR_indexes = range(train_size)\n",
        "    VL_indexes = range(train_size, train_size + validation_size)\n",
        "    TS_indexes = range(train_size + validation_size, train_size + validation_size + len(test_dataset))\n",
        "\n",
        "    return dataset, Nu, TR_indexes, VL_indexes, TS_indexes\n",
        "\n",
        "# Test the load_MNIST function\n",
        "dataset, Nu, TR_indexes, VL_indexes, TS_indexes = load_MNIST()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1KXqFiEm9ev"
      },
      "source": [
        "# Extra Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zNjNEq0Um9ev"
      },
      "outputs": [],
      "source": [
        "def select_indexes(data, indexes):\n",
        "\n",
        "    if len(data) == 1:\n",
        "        return [data[0]]\n",
        "\n",
        "    return [data[i] for i in indexes]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxufLpHZm9ev"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFEse8sqm9ew",
        "outputId": "9611c0ef-4a1d-46c7-c9dc-9c66984ebe7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "First train sample shape: torch.Size([1, 28, 28])\n",
            "First train label: 5\n",
            "Training dataset size: 48000\n",
            "Validation dataset size: 12000\n",
            "First input sample shape: torch.Size([10, 28, 28])\n",
            "First target sample shape: torch.Size([1])\n",
            "Total dataset size (after adding validation): 60000\n",
            "Total dataset size (after adding test): 70000\n",
            "input shape: torch.Size([70000, 10, 28, 28])\n",
            "target shape: torch.Size([70000, 1])\n",
            "torch.Size([70000, 10, 28, 28])\n",
            "Number of Calculated:  0\n",
            "Number of Calculated:  100\n",
            "Number of Calculated:  200\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m net \u001b[38;5;241m=\u001b[39m Readout(Nr, Nl, configs)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Compute states for the entire dataset\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m spikes, states \u001b[38;5;241m=\u001b[39m \u001b[43mdeepESN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputeState\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m states \u001b[38;5;241m=\u001b[39m states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     25\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_printoptions(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
            "Cell \u001b[1;32mIn[8], line 88\u001b[0m, in \u001b[0;36mDeepESN.computeState\u001b[1;34m(self, inputs, initialStates)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# print(self.W)\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# print(self.Win)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)):\n\u001b[1;32m---> 88\u001b[0m     spike, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputeGlobalState\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_seq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     spikes\u001b[38;5;241m.\u001b[39mappend(spike)\n\u001b[0;32m     90\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(state)\n",
            "Cell \u001b[1;32mIn[8], line 109\u001b[0m, in \u001b[0;36mDeepESN.computeGlobalState\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    107\u001b[0m     initialStatesLayer \u001b[38;5;241m=\u001b[39m state[layer\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNr: (layer\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNr]\n\u001b[0;32m    108\u001b[0m     initialStatesSpike \u001b[38;5;241m=\u001b[39m spike[layer\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNr: (layer\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNr]\n\u001b[1;32m--> 109\u001b[0m     state[layer\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNr:(layer\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNr], spike[layer\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNr:(layer\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNr] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomputeLayerState\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsedinput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitialStatesSpike\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitialStatesLayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     parsedinput \u001b[38;5;241m=\u001b[39m state[layer \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNr:(layer \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNr]\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m: \n",
            "Cell \u001b[1;32mIn[8], line 74\u001b[0m, in \u001b[0;36mDeepESN.computeLayerState\u001b[1;34m(self, input, layer, initialStatesSpike, initialStatesLayer)\u001b[0m\n\u001b[0;32m     71\u001b[0m spk \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNr, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Determine where the state exceeds the threshold\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m spk \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Reset the state values that exceed the threshold to 0\u001b[39;00m\n\u001b[0;32m     77\u001b[0m state[state \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold] \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the CIFAR-10 dataset and related configurations\n",
        "dataset, Nu, TR_indexes, VL_indexes, TS_indexes = load_MNIST()\n",
        "print(dataset.inputs.shape)\n",
        "# Load configuration for CIFAR-10 task\n",
        "configs = config_CIFAR10(list(TR_indexes) + list(VL_indexes))\n",
        "\n",
        "# Define parameters for DeepESN\n",
        "Nr = 400  # Number of recurrent units\n",
        "Nl = 2    # Number of recurrent layers\n",
        "reg = 0\n",
        "\n",
        "# Create an instance of DeepESN\n",
        "deepESN = DeepESN(Nu, Nr, Nl, configs, device)\n",
        "net = Readout(Nr, Nl, configs)\n",
        "\n",
        "# Compute states for the entire dataset\n",
        "spikes, states = deepESN.computeState(dataset.inputs.to(device))\n",
        "states = states.to('cpu') \n",
        "\n",
        "\n",
        "torch.set_printoptions(threshold=float('inf'))\n",
        "print(spikes[0])\n",
        "print(states[0])\n",
        "\n",
        "# Select training and test states and targets using their respective indexes\n",
        "train_states = select_indexes(spikes, list(TR_indexes) + list(VL_indexes))\n",
        "test_states = select_indexes(spikes, TS_indexes)\n",
        "train_targets = select_indexes(dataset.targets, list(TR_indexes) + list(VL_indexes))\n",
        "test_targets = select_indexes(dataset.targets, TS_indexes)\n",
        "\n",
        "# train_states = select_indexes(states, list(TR_indexes)[0:800])\n",
        "# train_targets = select_indexes(dataset.targets, list(TR_indexes)[0:800])\n",
        "# test_states = select_indexes(states, list(TR_indexes)[800:1000])\n",
        "# test_targets = select_indexes(dataset.targets, list(TR_indexes)[800:1000])\n",
        "\n",
        "# Reshape train_targets and test_targets to the required dimensions\n",
        "train_targets = torch.tensor(train_targets)\n",
        "# train_targets = train_targets.reshape(800, 1)\n",
        "test_targets = torch.tensor(test_targets)\n",
        "# test_targets = test_targets.reshape(200, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  1500] loss: 1.474\n",
            "[1,  3000] loss: 0.929\n",
            "[2,  1500] loss: 0.792\n",
            "[2,  3000] loss: 0.720\n",
            "[3,  1500] loss: 0.668\n",
            "[3,  3000] loss: 0.630\n",
            "[4,  1500] loss: 0.605\n",
            "[4,  3000] loss: 0.579\n",
            "[5,  1500] loss: 0.565\n",
            "[5,  3000] loss: 0.547\n",
            "[6,  1500] loss: 0.537\n",
            "[6,  3000] loss: 0.523\n",
            "[7,  1500] loss: 0.517\n",
            "[7,  3000] loss: 0.504\n",
            "[8,  1500] loss: 0.501\n",
            "[8,  3000] loss: 0.489\n",
            "[9,  1500] loss: 0.487\n",
            "[9,  3000] loss: 0.478\n",
            "[10,  1500] loss: 0.476\n",
            "[10,  3000] loss: 0.468\n",
            "Finished Training\n",
            "tensor([[[0., 0., 0., 0., 0., 1., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 1., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 1., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 1., 0., 1., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 1., 0., 0.]]], grad_fn=<SliceBackward0>)\n",
            "Accuracy: 0.8532\n"
          ]
        }
      ],
      "source": [
        "lr = 5*(10**-5)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "criterion = SF.loss.ce_count_loss()\n",
        "    \n",
        "num_epochs = 10\n",
        "batch_size = 20\n",
        "    \n",
        "    # Training Session\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for iter in range(int(60000/batch_size)):\n",
        "        iter_spike = torch.stack(train_states[iter*batch_size : (iter+1)*batch_size])\n",
        "        iter_target = train_targets[iter*batch_size : (iter+1)*batch_size]\n",
        "        \n",
        "        net.zero_grad()\n",
        "        outputs, _ = net(iter_spike)\n",
        "        loss = criterion(outputs, iter_target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    \n",
        "        if(iter + 1) % 1500 == 0:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, iter + 1, running_loss / 1500))\n",
        "            running_loss = 0.0\n",
        "        \n",
        "print('Finished Training')\n",
        "    \n",
        "#Testing 10,000 Data from Dataset\n",
        "\n",
        "outputs, _ = net(torch.stack(test_states))\n",
        "print(outputs[:,0:1,:])\n",
        "acc_rate = SF.acc.accuracy_rate(outputs, test_targets)\n",
        "    \n",
        "print(f\"Accuracy: {acc_rate}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
